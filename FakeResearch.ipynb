{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FakeResearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip0DPbAlfvtQ"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypad1-pvfvYh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "f15f0bc3-77ae-41fd-f076-064d8a06a9ed"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/Projects_Colab/Fake/englishcorona.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-04fc3aead2d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Projects_Colab/Fake/englishcorona.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /content/drive/My Drive/Projects_Colab/Fake/englishcorona.csv does not exist: '/content/drive/My Drive/Projects_Colab/Fake/englishcorona.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poFgYVLtfvUA"
      },
      "source": [
        "df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWQ9ewItfvPr"
      },
      "source": [
        "b1=list(set(list(df[\"0\"])))\n",
        "b2=[]\n",
        "len(b1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vu0SkYtoji7"
      },
      "source": [
        "hi = b1\n",
        "g = []\n",
        "for i in b1:\n",
        "    if (\"_id\") in i :\n",
        "        g.append(hi.index(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv5rJ6cBMI9w"
      },
      "source": [
        "g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Sp4jKipMO1i"
      },
      "source": [
        "b1[362]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN-0433Lidjz"
      },
      "source": [
        "d=b1\n",
        "b3=[]\n",
        "for i in range(len(d)):\n",
        "    if(d[i].find('_id')==-1):\n",
        "        b3.append(d[i])\n",
        "        \n",
        "b3 = list(set(b3))\n",
        "len(b3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJfq42gTpkM0"
      },
      "source": [
        "df = pd.DataFrame({\"Articles\":b3})\n",
        "df.to_csv(\"englishcorona.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93-yZR7CXN5Y"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e6meVxGU-SA"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owmntnwk5HGE"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "#from HindiTokenizer import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_haMMOmwbwQ"
      },
      "source": [
        "path = \"/content/drive/My Drive/Projects_Colab/Fake/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpAtECLTwdKs"
      },
      "source": [
        "Fakefile = path + \"f1.csv\"\n",
        "Truefile = \"englishcorona.csv\"\n",
        "Fakehindi= path+ \"hindicorona.csv\"\n",
        "Truehindi = \"Cleaned_hindi_news.csv\"\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sm9gWdkqY-g"
      },
      "source": [
        "Truehindi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAbLklOCqpHx"
      },
      "source": [
        "d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QNZTndHu5G0"
      },
      "source": [
        "# Concatenation of all the files \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZNS6t4mqyFa"
      },
      "source": [
        "b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7QzCte2uWx2"
      },
      "source": [
        "c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRRljyo0uZ1r"
      },
      "source": [
        "len(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6uJiTXe5EnM"
      },
      "source": [
        "\n",
        "a = pd.read_csv(Fakefile)\n",
        "\n",
        "b = pd.read_csv(Truefile)\n",
        "c= pd.read_csv(Fakehindi)\n",
        "d=pd.read_csv(Truehindi)\n",
        "fakehindi=c[\"0\"]\n",
        "fake = a[\"0\"]\n",
        "true = b[\"Articles\"]\n",
        "truehindi=d[\"Article\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0Z4V4h3u4Xz"
      },
      "source": [
        "len(fakehindi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgO1fePOvC11"
      },
      "source": [
        "len(truehindi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4f2aNKjMnZv"
      },
      "source": [
        "d[\"Body\"][507][500:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wVLeSRSsgjA"
      },
      "source": [
        "truehindi=list(set(truehindi))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyT8ZIXiK_YO"
      },
      "source": [
        "len(truehindi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skmXVri3RtBv"
      },
      "source": [
        "df_fake = pd.DataFrame({'Article':fake})\n",
        "df_true = pd.DataFrame({'Article':true})\n",
        "df_true['category'] = 1\n",
        "df_fake['category'] = 0\n",
        "df_true.to_csv(\"NewTrue.csv\",index=False)\n",
        "df_fake.to_csv(\"NewFake.csv\",index=False)\n",
        "df_fake.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01GeQRXJShFD"
      },
      "source": [
        "new_articles = list(df_true['Article'])\n",
        "category1 = list(df_true['category'])\n",
        "category2 = list(df_fake['category'])\n",
        "new_articles.extend(list(df_fake['Article']))\n",
        "df = pd.DataFrame({'Article':new_articles,'Category':category1 + category2})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpQyXIrtZ5jx"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYYy__ci4F8y"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1g4WhJ3TRZC"
      },
      "source": [
        "len(true),len(fake)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLHnDnQkzLs6"
      },
      "source": [
        "# **Data Cleaning**\n",
        "\n",
        "\n",
        "---\n",
        "This step involves cleaning of unwanted data that includes :-\n",
        "\n",
        "\n",
        "*   Instagram Tags\n",
        "*   News article sources\n",
        "\n",
        "\n",
        "*   Emojies\n",
        "*   Advertisements\n",
        "\n",
        "\n",
        "*   Tweets\n",
        "*   Dates\n",
        "\n",
        "*   Links\n",
        "\n",
        "*   Email\n",
        "*   urls\n",
        "\n",
        "\n",
        "*   Numbers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxaiOMDwabef"
      },
      "source": [
        "  df2 = df\n",
        "  #df2 = df2.replace(r'@[.\\w]{5,}',' ', regex=True)\n",
        "  #df2 = df2.replace(r'#[.\\w]{5,}',' ', regex=True)\n",
        "  df2 = df2.replace(r'[.\\w]{3,}@[.\\w]{5,}','', regex=True)\n",
        "  df2 = df2.replace(r'https?://[.\\w]{3,}','', regex=True)\n",
        "  df2 = df2.replace(r'www.[.\\w]{3,}','', regex=True)\n",
        "  df2 = df2.replace(r'[0-9]','', regex=True)\n",
        "  df2 = df2.replace(r'\\xa0',' ', regex=True)\n",
        "  #df2 = df2.replace(r'\\u','', regex=True)\n",
        "  df2 = df2.replace(r'\\n',' ', regex=True)\n",
        "  df2 = df2.replace(r'\\t','', regex=True)\n",
        "  df2 = df2.replace(r'\\r','', regex=True)\n",
        "  df2 = df2.replace(r'\\s\\s\\s','', regex=True)\n",
        "  #df2 = df2.replace(r'[^\\w\\s]','',regex=True)\n",
        "\n",
        "        #print(i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoDyjLDL5bYl"
      },
      "source": [
        "df=df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssig-lFkwdiA"
      },
      "source": [
        "df2.to_csv(\"cleaned.csv\")   #cleaned file is stored in the csv file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XXB6EnbxF9Y"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = []\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "for i in range(0,844 ):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', df['Article'][i])\n",
        "    #review = str(df['Article'][i])\n",
        "    review = review.lower()\n",
        "    review = nltk.word_tokenize(review)\n",
        "    #ps = PorterStemmer()\n",
        "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJrPATdIzpXM"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXIhD3Ut2uX-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm9Zt_lD2pTP"
      },
      "source": [
        "def vectorize_text(features, max_features):\n",
        "    vectorizer = TfidfVectorizer( stop_words='english',\n",
        "                            decode_error='strict',\n",
        "                            analyzer='word',\n",
        "                            ngram_range=(1, 2),\n",
        "                            max_features=max_features\n",
        "                            #max_df=0.5 # Verwendet im ML-Kurs unter Preprocessing                   \n",
        "                            )\n",
        "    \n",
        "    feature_vec = vectorizer.fit_transform(features)\n",
        "    print(vectorizer.get_feature_names)\n",
        "    return feature_vec.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiex9Ncv2vOh"
      },
      "source": [
        "tfidf_features = vectorize_text(corpus,5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CFmwbbV2-Om"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8PnS2nV6ChW"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1600)\n",
        "\n",
        "X = vectorizer.fit_transform(corpus).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqXsD6Fv0CHP"
      },
      "source": [
        "sklearn_tfidf = TfidfVectorizer(ngram_range= (3,3),stop_words=stopwordslist, norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
        "sklearn_representation = sklearn_tfidf.fit_transform(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITVwhhRr3jST"
      },
      "source": [
        "X=tfidf_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2xqtMN87ZSx"
      },
      "source": [
        "y = df2['Category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yvYLhKA8lKD"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgauuIKBZl0g"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(C=1e5)\n",
        "logreg.fit(X_train, y_train)\n",
        "pred = logreg.predict(X_test)\n",
        "print('Accuracy of Lasso classifier on training set: {:.2f}'\n",
        "     .format(logreg.score(X_train, y_train)))\n",
        "print('Accuracy of Lasso classifier on test set: {:.2f}'\n",
        "     .format(logreg.score(X_test, y_test)))\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn2Gtcc68eqJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPh5A67z7pFS"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCkTYSMT8zfA"
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_pred,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcyCCCvEY3F_"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzRnl6_t9G8V"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel=\"rbf\")\n",
        "classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YRglMq89KRT"
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_pred,y_test))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSsa7eDa9WsR"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel=\"linear\")\n",
        "classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt61fNWJ9Wqi"
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_pred,y_test))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXJssZUz9Wmo"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "early_stop = EarlyStopping(monitor='val_loss',patience=1.5)\n",
        "N, D = X_train.shape\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Input(shape=(D,)),\n",
        "  tf.keras.layers.Dense(300,activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.7),\n",
        "  tf.keras.layers.Dense(50,activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.7),\n",
        "  tf.keras.layers.Dense(50,activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.7),\n",
        "  #tf.keras.layers.Dense(50,activation='relu'),\n",
        "  #tf.keras.layers.Dense(10,activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(Adam(lr=0.005),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "his = model.fit(X_train,y_train,epochs=25,verbose=1,shuffle=1,validation_split=0.1,callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IN6aFbx9WjI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(his.history['loss'])\n",
        "plt.plot(his.history['val_loss'])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend([\"Loss\",\"Val_loss\"])\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.savefig('plot4.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy\")\n",
        "print(accuracy_score(y_pred,y_test))\n",
        "print(\"Confusion matrix\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqbcawjY9Wgr"
      },
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers.embeddings import Embedding\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9UxUVMrkm3E"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "#X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyMTgKIx5x05"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1umrB9y06h9F"
      },
      "source": [
        "! git clone http://github.com/stanfordnlp/glove\n",
        "! cd glove && make\n",
        "! ./demo.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8fgoG_m6rDT"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf855WWHvwNa"
      },
      "source": [
        "# Data Preprocessinh\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Removing punctuation\n",
        "*   Tokenization\n",
        "\n",
        "\n",
        "*   Stemming\n",
        "*   Stopwords\n",
        "\n",
        "*   Made a new csv file with final data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhj4nFwmvv0V"
      },
      "source": [
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glNngURTwdmw"
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')\n",
        "\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "\n",
        "def remove_punct(my_str):\n",
        "  no_punct = \"\"\n",
        "  for char in my_str:\n",
        "    if char not in punctuations:\n",
        "        no_punct = no_punct + char\n",
        "  return no_punct\n",
        "\n",
        "def generate_stem_words(word):\n",
        "\t\tsuffixes = {\n",
        "    1: [u\"ो\",u\"े\",u\"ू\",u\"ु\",u\"ी\",u\"ि\",u\"ा\"],\n",
        "    2: [u\"कर\",u\"ाओ\",u\"िए\",u\"ाई\",u\"ाए\",u\"ने\",u\"नी\",u\"ना\",u\"ते\",u\"ीं\",u\"ती\",u\"ता\",u\"ाँ\",u\"ां\",u\"ों\",u\"ें\"],\n",
        "    3: [u\"ाकर\",u\"ाइए\",u\"ाईं\",u\"ाया\",u\"ेगी\",u\"ेगा\",u\"ोगी\",u\"ोगे\",u\"ाने\",u\"ाना\",u\"ाते\",u\"ाती\",u\"ाता\",u\"तीं\",u\"ाओं\",u\"ाएं\",u\"ुओं\",u\"ुएं\",u\"ुआं\"],\n",
        "    4: [u\"ाएगी\",u\"ाएगा\",u\"ाओगी\",u\"ाओगे\",u\"एंगी\",u\"ेंगी\",u\"एंगे\",u\"ेंगे\",u\"ूंगी\",u\"ूंगा\",u\"ातीं\",u\"नाओं\",u\"नाएं\",u\"ताओं\",u\"ताएं\",u\"ियाँ\",u\"ियों\",u\"ियां\"],\n",
        "    5: [u\"ाएंगी\",u\"ाएंगे\",u\"ाऊंगी\",u\"ाऊंगा\",u\"ाइयाँ\",u\"ाइयों\",u\"ाइयां\"],\n",
        "}\n",
        "  \n",
        "\t\tfor L in 5, 4, 3, 2, 1:\n",
        "\t\t\tif len(word) > L + 1:\n",
        "\t\t\t\tfor suf in suffixes[L]:\n",
        "\t\t\t\t\t#print type(suf),type(word),word,suf\n",
        "\t\t\t\t\tif word.endswith(suf):\n",
        "\t\t\t\t\t\t#print 'h'\n",
        "\t\t\t\t\t\treturn word[:-L]\n",
        "\t\treturn word\n",
        "def generate(text):\n",
        "  new=[]\n",
        "  for w in text:\n",
        "    word = generate_stem_words(w)\n",
        "    new.append(word)\n",
        "  return \" \".join(new)\n",
        "with open(path+\"stopwords.txt\") as f:\n",
        "  all_stopwords = f.read().split(\"\\n\")\n",
        "\n",
        "def remove_stop_words(text_list):\n",
        "  new = [text for text in text_list if not (text in all_stopwords)]\n",
        "  return \" \".join(new)\n",
        "hi = remove_stop_words(nltk.word_tokenize(df2['Article'][0]))\n",
        "\n",
        "def preprocess(df):\n",
        "  df['punctuated'] = df['Article'].apply(remove_punct)\n",
        "  df['stemmed'] = df['punctuated'].apply(lambda x:generate(nltk.word_tokenize(x)))\n",
        "  df['stopwords'] = df['stemmed'].apply(lambda x:remove_stop_words(nltk.word_tokenize(x)))\n",
        "  return df\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn4s2tk8S-F7"
      },
      "source": [
        "df3 = df2\n",
        "df1 = preprocess(df3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsqC8XLONvEf"
      },
      "source": [
        "df1['punctuated'][5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fne_ncAzXpjc"
      },
      "source": [
        "len(df1['stemmed'][5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqEQ4oQeXqCx"
      },
      "source": [
        "len(df1['stopwords'][5])\n",
        "df1['Category'] = df['Category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqcNs_5_cGjK"
      },
      "source": [
        "df1.to_csv(\"Final.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX6IdtCOwNVg"
      },
      "source": [
        "# Vectorization (Creating bag of words)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIdMBPlFYMfX"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(\n",
        "                            \n",
        "                            analyzer='word',\n",
        "                            ngram_range=(1, 3),\n",
        "                            max_features=5000\n",
        "                            #max_df=0.5 # Verwendet im ML-Kurs unter Preprocessing                   \n",
        "                            )\n",
        "\n",
        "X = vectorizer.fit_transform(df['stopwords']).toarray() # remember to use the original X_train set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJJs0ujrwWv0"
      },
      "source": [
        "## Creating Train and Test split "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRnTGn23dkl9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,df1['Category'],test_size=0.33,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aom-5-SwdvL"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds6Zk9AlwgGo"
      },
      "source": [
        "# Running Models \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNS6qGWCwd45"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFZHmgVZwd2l"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_pred,y_test))\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm\n",
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import  f1_score,precision_score,recall_score\n",
        "acc = accuracy_score(y_test,y_pred)\n",
        "prec = precision_score(y_test,y_pred)\n",
        "rec = recall_score(y_test,y_pred)\n",
        "f1 = f1_score(y_test,y_pred)\n",
        "results=pd.DataFrame([['MultinomialNB',acc,prec,rec,f1]],columns=['Model','Accuracy','Precision','Recall','F1 Score'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S4ffR7E_Sd1"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c0QY_OOwndU"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(C=1e5)\n",
        "logreg.fit(X_train, y_train)\n",
        "pred = logreg.predict(X_test)\n",
        "print('Accuracy  on training set: {:.2f}'\n",
        "     .format(logreg.score(X_train, y_train)))\n",
        "print('Accuracy on test set: {:.2f}'\n",
        "     .format(logreg.score(X_test, y_test)))\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "cm\n",
        "y_pred = classifier.predict(X_test)\n",
        "#from sklearn.metrics import  confusion_matrix,accuracy_score,f1_score,precision_score,recall_score\n",
        "acc = accuracy_score(y_test,y_pred)\n",
        "prec = precision_score(y_test,y_pred)\n",
        "rec = recall_score(y_test,y_pred)\n",
        "f1 = f1_score(y_test,y_pred)\n",
        "\n",
        "model_results=pd.DataFrame([['Logistic Regression',acc,prec,rec,f1]],columns=['Model','Accuracy','Precision','Recall','F1 Score'])\n",
        "results = results.append(model_results,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y68L7Fae_ir5"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgcVoTQ8wdzz"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier = SVC()\n",
        "classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_LbpIhuwdyY"
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_pred,y_test))\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "cm\n",
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import  confusion_matrix,accuracy_score,f1_score,precision_score,recall_score\n",
        "acc = accuracy_score(y_test,y_pred)\n",
        "prec = precision_score(y_test,y_pred)\n",
        "rec = recall_score(y_test,y_pred)\n",
        "f1 = f1_score(y_test,y_pred)\n",
        "\n",
        "model_results=pd.DataFrame([['SVM  ',acc,prec,rec,f1]],columns=['Model','Accuracy','Precision','Recall','F1 Score'])\n",
        "results = results.append(model_results,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTx9IWxjwds1"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train,y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwXOclD3wdp1"
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_pred,y_test))\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD05Zd__wdGg"
      },
      "source": [
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "classifier1 = PassiveAggressiveClassifier()\n",
        "classifier1.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyduK0I0wdDX"
      },
      "source": [
        "y_pred = classifier1.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_pred,y_test))\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "cmy_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import  confusion_matrix,accuracy_score,f1_score,precision_score,recall_score\n",
        "acc = accuracy_score(y_test,y_pred)\n",
        "prec = precision_score(y_test,y_pred)\n",
        "rec = recall_score(y_test,y_pred)\n",
        "f1 = f1_score(y_test,y_pred)\n",
        "\n",
        "model_results=pd.DataFrame([['Passive Agressive Classifier ',acc,prec,rec,f1]],columns=['Model','Accuracy','Precision','Recall','F1 Score'])\n",
        "results = results.append(model_results,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq1U8cyyiGaw"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "early_stop = EarlyStopping(monitor='val_loss',patience=1.5)\n",
        "N, D = X_train.shape\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Input(shape=(D,)),\n",
        "  tf.keras.layers.Dense(300,activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.6),\n",
        "  tf.keras.layers.Dense(50,activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.7),\n",
        "  tf.keras.layers.Dense(50,activation='relu'),\n",
        "  #tf.keras.layers.Dense(10,activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(Adam(lr=0.00045),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "his = model.fit(X_train,y_train,epochs=25,verbose=1,shuffle=1,validation_split=0.1,callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atnZmup_k28o"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(his.history['loss'])\n",
        "plt.plot(his.history['val_loss'])\n",
        "plt.show()\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "cm\n",
        "y_pred=model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFaxhXNbyzUG"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2).fit(X)\n",
        "data2D = pca.transform(X)\n",
        "plt.scatter(data2D[:,0], data2D[:,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NiZpKNZoOYs"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_train, y_train\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
        "plt.title('Logistic Regression (Training set)')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4EN6MSFz5pk"
      },
      "source": [
        "plt.scatter(y_pred,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfIX6BaB0tGT"
      },
      "source": [
        "def build_corpus(data):\n",
        "    \"Creates a list of lists containing words from each sentence\"\n",
        "    corpus = []\n",
        "    for col in data:\n",
        "        \n",
        "            corpus.append(col.split())\n",
        "\n",
        "    return corpus\n",
        "\n",
        "corpus1 = build_corpus(corpus)\n",
        "corpus1[0:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIAJ2MIy1rTf"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "#model = word2vec.Word2Vec(corpus1, size=100, window=20, min_count=200, workers=4)\n",
        "model = word2vec.Word2Vec(corpus1, size=100, window=20, min_count=100, workers=4)\n",
        "def tsne_plot(model):\n",
        "    \"Creates and TSNE model and plots it\"\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    for word in model.wv.vocab:\n",
        "        tokens.append(model[word])\n",
        "        labels.append(word)\n",
        "\n",
        "    tsne_model = TSNE(perplexity=500, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
        "    new_values = tsne_model.fit_transform(tokens)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "\n",
        "    plt.figure(figsize=(16, 16)) \n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.savefig('bow1.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "tsne_plot(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKZBQkgm27nA"
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import  accuracy_score,f1_score,precision_score,recall_score\n",
        "acc = accuracy_score(y_test,y_pred)\n",
        "prec = precision_score(y_test,y_pred)\n",
        "rec = recall_score(y_test,y_pred)\n",
        "f1 = f1_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}